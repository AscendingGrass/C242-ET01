{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AscendingGrass/C242-ET01/blob/main/C242_ET01_Llama_3_1_8b_%2B_Unsloth_2x_faster_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save) (eg for Llama.cpp).\n",
        "\n",
        "[NEW] Llama-3.1 8b, 70b & 405b are trained on a crazy 15 trillion tokens with 128K long context lengths!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install unsloth \"xformers==0.0.28.post2\"\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2v_X2fA0Df5"
      },
      "source": [
        "* We support Llama, Mistral, Phi-3, Gemma, Yi, DeepSeek, Qwen, TinyLlama, Vicuna, Open Hermes etc\n",
        "* We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n",
        "* `max_seq_length` can be set to anything, since we do automatic RoPE Scaling via [kaiokendev's](https://kaiokendev.github.io/til) method.\n",
        "* [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "* [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "* [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
      ],
      "metadata": {
        "id": "-b192iONBEO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "9238a77a-f268-41f6-ee2a-36a122e15e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "saved_model_names = [\n",
        "    \"AscendingGrass/Llama-3.2_3B_C242-ET01\",\n",
        "    \"AscendingGrass/Llama-3.1_8B_C242-ET01\"\n",
        "]\n",
        "\n",
        "run = 0\n",
        "\n",
        "base_model = fourbit_models[run]\n",
        "saved_model_name = saved_model_names[run]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "808f08c5-902b-44b4-c35b-85bc3ff8d252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.12.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    # use_dora=True,\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the Alpaca dataset from [yahma](https://huggingface.co/datasets/yahma/alpaca-cleaned), which is a filtered version of 52K of the original [Alpaca dataset](https://crfm.stanford.edu/2023/03/13/alpaca.html). You can replace this code section with your own data prep.\n",
        "\n",
        "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
        "\n",
        "**[NOTE]** Remember to add the **EOS_TOKEN** to the tokenized output!! Otherwise you'll get infinite generations!\n",
        "\n",
        "If you want to use the `llama-3` template for ShareGPT datasets, try our conversational [notebook](https://colab.research.google.com/drive/1XamvWYinY6FOSX9GLvnqSjjsNflxdhNc?usp=sharing).\n",
        "\n",
        "For text completions like novel writing, try this [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "collapsed": true,
        "outputId": "7544074b-3eac-42cb-a55c-fffe6a355f95"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c13c5c682dd8>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m {}\"\"\"\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mEOS_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m \u001b[0;31m# Must add EOS_TOKEN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mformatting_prompts_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0minstructions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"instruction\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "# ### Instruction:\n",
        "# {}\n",
        "\n",
        "# ### Input:\n",
        "# {}\n",
        "\n",
        "# ### Response:\n",
        "# {}\"\"\"\n",
        "\n",
        "# EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "# def formatting_prompts_func(examples):\n",
        "#     instructions = examples[\"instruction\"]\n",
        "#     inputs       = examples[\"input\"]\n",
        "#     outputs      = examples[\"output\"]\n",
        "#     texts = []\n",
        "#     for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "#         # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "#         text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "#         texts.append(text)\n",
        "#     return { \"text\" : texts, }\n",
        "# pass\n",
        "\n",
        "# from datasets import load_dataset\n",
        "# # dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
        "# dataset = datasets.load_dataset(\"SEACrowd/squad_id\", trust_remote_code=True)\n",
        "# # dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llama3_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{}\"\"\"\n",
        "\n",
        "system_prompt=\"\"\"Gunakan bahasa indonesia. Anda adalah pembuat soal yang dapat membuat soal yang relevan berdasarkan materi yang ada. Anda harus mengakses informasi dari dokumen yang sesuai dan menghasilkan soal berkualitas tinggi, termasuk soal HOTS. Jika tidak diminta membuat soal maka hiraukan saja teks setelah ini.\n",
        "\\\"\\\"\\\"Berikut adalah instruksi untuk menghasilkan soal:\n",
        "\n",
        "1. **Akses Materi**:\n",
        "   - Akses informasi dari telah tersedia pada pesan chat yang berkaitan dengan modul yang diminta. Pastikan Anda mengambil data yang relevan tentang topik ini. Jika ditanya perihal konteks anda dapat mengambil data dari vector store tool.\n",
        "\n",
        "1. **Buat Soal Pilihan Ganda (Multiple Choice)**:\n",
        "   - Berdasarkan data yang diambil, buat soal pilihan ganda dengan 4-5 pilihan jawaban yang relevan.\n",
        "   - Pastikan soal tidak hanya menguji pengetahuan dasar, melainkan menguji aplikasi, analisis, dan sintesis konsep. Hindari membuat soal berbentuk esai atau pertanyaan terbuka.\n",
        "\n",
        "2. **Buat Soal Benar/Salah (True or False)**:\n",
        "   - Gunakan informasi yang diambil untuk membuat soal dengan format benar/salah yang menguji definisi, konsep, atau aplikasi dari materi yang dipelajari.\n",
        "   - Pastikan pernyataan yang diberikan dapat dinilai jelas apakah benar atau salah, dan berikan penjelasan singkat setelah jawaban untuk memperjelas.\n",
        "\n",
        "3. **Buat Soal Multi-Box Choice (Multiple Correct Answers)**:\n",
        "   - Berdasarkan informasi yang ada, buat soal multi-box choice di mana lebih dari satu jawaban yang benar. Pastikan soal tersebut jelas menunjukkan bahwa ada lebih dari satu pilihan yang benar dan siswa diminta untuk memilih lebih dari satu jawaban yang tepat.\n",
        "   - Tentukan dengan jelas berapa banyak jawaban yang benar dalam soal tersebut. Jawaban benar terdiri dari 1-5 dari jumlah pilihan 4-5.\n",
        "\n",
        "4. **Fokus pada HOTS (Higher Order Thinking Skills)**:\n",
        "   - Soal HOTS hanya dapat berupa Multiple Choice, Multi-Box Choice, dan True/False.\n",
        "   - Gunakan data yang relevan untuk menciptakan soal yang mendorong siswa untuk berpikir kritis, menganalisis, dan memberikan solusi berdasarkan pemahaman mereka terhadap konsep yang diajarkan.\n",
        "   - Pastikan soal yang dibuat menguji pemahaman aplikatif dan bukan hanya konsep dasar atau teori.\n",
        "\n",
        "5. **Verifikasi Keakuratan Soal**:\n",
        "   - Setelah soal dibuat, pastikan setiap jawaban sesuai dengan data yang ada. Pastikan bahwa setiap pilihan jawaban atau pernyataan terkait dengan konsep yang benar dan terperinci dari materi yang relevan. Lalu berikan penjelasan diakhir masing-masing soal.\n",
        "\n",
        "6. **Variasikan Tingkat Kesulitan**:\n",
        "   - Sesuaikan soal dengan tingkat kesulitan yang berbeda, mulai dari soal dasar yang menguji pemahaman konsep hingga soal yang mengharuskan siswa untuk menganalisis atau mengaplikasikan pengetahuan mereka dalam konteks yang lebih kompleks.\n",
        "\n",
        "7. **Jumlah Soal**:\n",
        "   - Jika tidak ada ketentuan jumlah soal, buatlah 5 soal dengan variasi jenis soal yang beragam (pilihan ganda, multi-box choice, dan benar/salah), memastikan soal yang dihasilkan mencakup pilihan ganda dan benar/salah, serta beberapa soal yang menguji keterampilan berpikir tingkat tinggi (HOTS).\n",
        "\n",
        "8. **Format menggunakan JSON**\n",
        "  - Berikut contohnya:\n",
        "{\n",
        "  \"questions\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"type\": \"multiple_choice\",\n",
        "      \"question\": \"Apa ibu kota Indonesia?\",\n",
        "      \"options\": [\n",
        "        { \"id\": \"a\", \"text\": \"Jakarta\" },\n",
        "        { \"id\": \"b\", \"text\": \"Surabaya\" },\n",
        "        { \"id\": \"c\", \"text\": \"Bandung\" },\n",
        "        { \"id\": \"d\", \"text\": \"Yogyakarta\" }\n",
        "      ],\n",
        "      \"correct_answer\": \"a\",\n",
        "      \"explanation\": \"Ibu kota Indonesia adalah Jakarta, yang merupakan pusat pemerintahan dan ekonomi negara.\"\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"type\": \"multibox\",\n",
        "      \"question\": \"Pilihlah warna primer!\",\n",
        "      \"options\": [\n",
        "        { \"id\": \"a\", \"text\": \"Merah\" },\n",
        "        { \"id\": \"b\", \"text\": \"Hijau\" },\n",
        "        { \"id\": \"c\", \"text\": \"Biru\" },\n",
        "        { \"id\": \"d\", \"text\": \"Kuning\" }\n",
        "      ],\n",
        "      \"correct_answer\": [\"a\", \"c\", \"d\"],\n",
        "      \"explanation\": \"Warna primer adalah warna yang tidak bisa dihasilkan dari pencampuran warna lain. Warna primer adalah merah, biru, dan kuning.\"\n",
        "    },\n",
        "    {\n",
        "      \"id\": 3,\n",
        "      \"type\": \"true_false\",\n",
        "      \"question\": \"Sungai Amazon terletak di Afrika.\",\n",
        "      \"correct_answer\": false,\n",
        "      \"explanation\": \"Sungai Amazon terletak di Amerika Selatan, bukan di Afrika.\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\n",
        "**Catatan**: Dilarang untuk membuat soal dalam bentuk esai, pertanyaan terbuka, atau format lain selain Multiple Choice, Multi-Box Choice, dan True/False. Semua soal harus berupa pilihan ganda, dengan atau tanpa lebih dari satu jawaban yang benar, dan harus mencakup soal HOTS.\n",
        "\\\"\\\"\\\"\"\"\"\n",
        "\n",
        "user_prompt=\"Jawab Pesan Saya ini \\\"{}\\\". anda wajib mengakses data dari vector store tool. Hiraukan data berikut ini jika saya tidak menyuruh anda untuk \\\"membuat soal\\\". Data materinya buat soal dari teks ini: \\\"\\\"\\\"{}\\\"\\\"\\\"\"\n",
        "\n",
        "question_generation_prompts = dict(\n",
        "    easy=\"Buat soal dengan tingkatan Mudah (Tingkat Analisis Kasus Mudah, Tingkat Hafalan Mudah)\",\n",
        "    medium=\"Buat soal dengan tingkatan Sedang (Tingkat Analisis Kasus Sedang, Tingkat Hafalan Sedang)\",\n",
        "    hard=\"Buat soal dengan tingkatan Sulit (Tingkat Analisis Kasus Sulit, Tingkat Hafalan Susah, Kompleksitas soal Sulit, gabungan beberapa materi)\"\n",
        ")\n",
        "\n",
        "EOS_TOKEN = \"<|eot_id|>\" # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    contexts     = examples[\"context\"]\n",
        "    difficulties  = examples[\"difficulty\"]\n",
        "    questions_list   = examples[\"questions\"]\n",
        "    texts = []\n",
        "    for context, difficulty, questions in zip(contexts, difficulties, questions_list):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = llama3_prompt.format(\n",
        "            system_prompt,\n",
        "            user_prompt.format(question_generation_prompts[difficulty], context),\n",
        "            questions\n",
        "        ) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "from datasets import load_dataset\n",
        "train_path = \"./train.csv\"\n",
        "test_path = \"./test.csv\"\n",
        "\n",
        "dataset_train = load_dataset(\"csv\", data_files=train_path, split=\"train\")\n",
        "dataset_train = dataset_train.map(formatting_prompts_func, batched = True)\n",
        "# dataset_test = load_dataset(\"csv\", data_files=test_path, split=\"test\")\n",
        "# dataset_test = dataset_test.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "dataset = dataset_train"
      ],
      "metadata": {
        "id": "H_JECL03YVEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: login and initialize wandb\n",
        "\n",
        "import wandb\n",
        "from google.colab import userdata\n",
        "\n",
        "wandb.login(key=userdata.get(\"WANDB_API_KEY\"))\n",
        "\n",
        "# Initialize a Weights & Biases run\n",
        "wandb.init(\n",
        "    project=\"C242-ET01\",\n",
        "    name=saved_model_name,\n",
        "    config=dict(\n",
        "        base_model=base_model,\n",
        "        saved_to=saved_model_name\n",
        "    )) # replace with your project name and username"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "mubwDIYyzkSt",
        "outputId": "f0297faf-52b2-4d6a-bf19-7c9e65fc7849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mascending-grass\u001b[0m (\u001b[33mAscendingGrass\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241207_083453-pk5hulu2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/AscendingGrass/C242-ET01/runs/pk5hulu2' target=\"_blank\">AscendingGrass/Llama-3.2_3B_C242-ET01</a></strong> to <a href='https://wandb.ai/AscendingGrass/C242-ET01' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/AscendingGrass/C242-ET01' target=\"_blank\">https://wandb.ai/AscendingGrass/C242-ET01</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/AscendingGrass/C242-ET01/runs/pk5hulu2' target=\"_blank\">https://wandb.ai/AscendingGrass/C242-ET01/runs/pk5hulu2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/AscendingGrass/C242-ET01/runs/pk5hulu2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e7d23e0bc70>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3, # Set this for 1 full training run.\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"wandb\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d2c81f-7bee-49c6-f50f-f8a2917c46c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.748 GB.\n",
            "2.635 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4f4d5c2d-89df-4059-9fcf-37d852e80d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 180 | Num Epochs = 3\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 66\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [66/66 53:49, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.173300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.158800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.128500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.060600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.999800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.909400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.844600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.768300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.558200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.544900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.558500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.543300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.503800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.465200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.450300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.437200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.417900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.485300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.493000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.393000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.443600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.666400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.385700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.440500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.335900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.400900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.326600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.458900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.350900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.410100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.291500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.456500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.446400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.353200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.355600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.379500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.360800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.441900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.324300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.344800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.582900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.329700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.303400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.367900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.324800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.309300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.336600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.338400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.368500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.301900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.346100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.380900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.324800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.330200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.330700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.280100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.336500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.286500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.314200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.279100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "9gvw6A6Hl7pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc49b1d6-584e-47b3-cce2-1d34a8bf1706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "475.9685 seconds used for training.\n",
            "7.93 minutes used for training.\n",
            "Peak reserved memory = 7.922 GB.\n",
            "Peak reserved memory for training = 1.938 GB.\n",
            "Peak reserved memory % of max memory = 53.716 %.\n",
            "Peak reserved memory for training % of max memory = 13.141 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08e806ed-01a8-47c9-cfc5-30f4139707a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765<|end_of_text|>']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# # alpaca_prompt = Copied from above\n",
        "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "# inputs = tokenizer(\n",
        "# [\n",
        "#     alpaca_prompt.format(\n",
        "#         \"Continue the fibonnaci sequence.\", # instruction\n",
        "#         \"1, 1, 2, 3, 5, 8\", # input\n",
        "#         \"\", # output - leave this blank for generation!\n",
        "#     )\n",
        "# ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "# tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d991da-e442-474a-b571-db199b6343d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Continue the fibonnaci sequence.\n",
            "\n",
            "### Input:\n",
            "1, 1, 2, 3, 5, 8\n",
            "\n",
            "### Response:\n",
            "13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765<|end_of_text|>\n"
          ]
        }
      ],
      "source": [
        "# # alpaca_prompt = Copied from above\n",
        "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "# inputs = tokenizer(\n",
        "# [\n",
        "#     alpaca_prompt.format(\n",
        "#         \"Continue the fibonnaci sequence.\", # instruction\n",
        "#         \"1, 1, 2, 3, 5, 8\", # input\n",
        "#         \"\", # output - leave this blank for generation!\n",
        "#     )\n",
        "# ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# text_streamer = TextStreamer(tokenizer)\n",
        "# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saved_model_name=\"AscendingGrass/TEST_REPO\""
      ],
      "metadata": {
        "id": "rdr2LdWf_cAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "a6065ab8a73847ec9c845f40bb0f1757",
            "ec15cecda6064d459e4ff317a2614b8b",
            "0349b41a0c3e489f87ac753edd48fad3",
            "14980788acca42b18ebe8d5c66047b46",
            "6f48269bbd3f4fa5a68ea5f9b422ae78",
            "a086aaec5a3643c38d0bd49ca3804f34",
            "44ad172cb4084ece94712960cba91daf",
            "683a7cba669f45b1b490b400a0fd688a",
            "9085cc7419d9451eb5cefb13ff2a6b13",
            "2169bef564a54629be459cd18a55fbe0",
            "9acbe1fd69294e6aaf25d0ad8594742d",
            "2d1e90a8554a4ce8986797e829df2822",
            "c6883d84680c4716b7c2b45921fffdf8",
            "9dfc5badcf7747e9a5a2caf6d9744692",
            "27e847a0657b4712ae4cc197f99e4e95",
            "ea842a8a52d8401e858a485fdd7a0531",
            "bf31cfaf8e2544d4b4cf541ef92a63da",
            "36a5c96e6c3c4c69a8c517a89822bb42",
            "d8714c6864484e42bc771749910d8f75",
            "8446e0526f534be18c39f1ea9f36794d",
            "508ceda9d28448c4b60983aa6b5e78af",
            "a0ccbf04155d4ccea78afa0fdaa7d070",
            "1e06d0a54f43430497b74e884ce0beb1",
            "159f4a611d8644cf8df08aa5fa319b9d",
            "f85835c0418148b292e18224084fee39",
            "58fb208e819a41388041b8efba349219",
            "5b84fbcc0bf44b3186de04c16c8b7f89",
            "214ece7fb1414f0a99647073a5cfe6f9",
            "1481bb391798485bbfd272df95ad3f64",
            "866c3c9bd00e4d2eaa61c8633c5af31f",
            "c3e2e6501f7e4ebf9450b8ae772a1ab8",
            "5709dabc11d643f18ee6e35209ac9b2f",
            "496c614a51204cb0915075e03f46d35f",
            "77ba741a2f924864a6bfe885f4155637",
            "51ba12b39d774d1aa693573ce715d1c6",
            "df64b328aabc41eabe6c13e2372e3752",
            "f677933bb4594481aa2d53894eed91d2",
            "f1de4d8520374823a63f83fba44cbd52",
            "a2ac1cbb86214038b0cc9465319ccbad",
            "ef39eb4d8c75467d875670742e47a2bf",
            "7f1ed120a76747b1837ecdd87f2e2cd9",
            "76dc7c67f3f748e689a8f8ccc3d15d17",
            "1be5dad10647480eb8d2ed7a5c4ba6b7",
            "8e9be764eb8243158726fc75419d4b22",
            "6b32fde2393d42c982f97ab33f3f5f6d",
            "ce5f3ad9eb5a44ed8a50389fc1121fa2",
            "68dab5985f964d3bb5c4de0246824e85",
            "807180bccb774df8ac40a0ed6e4cbdd6",
            "97eff5ab531e4c22a503274ebf178d6f",
            "6d7379a8d2654233a8175032ad985196",
            "16fe301a1b934b20884dc6ce1bea32b8",
            "1b3a831bc32943dd90f5eb012cccd3bf",
            "925a769a27db4979bb0ab18b311c0de9",
            "da1b40b7ae0e46d2b96bd62be3216d50",
            "d58939c751884a60a06c734391f0cf3a"
          ]
        },
        "outputId": "66182a4b-09bb-44e1-db6b-1339243532f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/603 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6065ab8a73847ec9c845f40bb0f1757"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d1e90a8554a4ce8986797e829df2822"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/97.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e06d0a54f43430497b74e884ce0beb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to https://huggingface.co/AscendingGrass/Llama-3.2_3B_C242-ET01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77ba741a2f924864a6bfe885f4155637"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b32fde2393d42c982f97ab33f3f5f6d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# model.save_pretrained(\"lora_model\") # Local saving\n",
        "# tokenizer.save_pretrained(\"lora_model\")\n",
        "model.push_to_hub(saved_model_name, token = userdata.get('HF_PUSH_TOKEN')) # Online saving\n",
        "tokenizer.push_to_hub(saved_model_name, token = userdata.get('HF_PUSH_TOKEN')) # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "d355bfd2795e47018fb303c4b19e818b",
            "01b378cd8347424bb7803b532890e267",
            "69f4879407ef431192ef98288b7a090c",
            "e9c876e3ece5492b9b4bc6e58f61556f",
            "f05b456fdd934f2dbcc354a336a9ac50",
            "1b79e193b15540e49b9ef7285e750512",
            "5493b0c290b04c7fb65a41438bcd59f4",
            "4b802be75d854969823d3c734154df17",
            "10b53a6ab9e44513b493294496c9819e",
            "88ce87a860db47829e1d4bf6220a12ca",
            "215a911336994f0c9af7429309bc2f1c"
          ]
        },
        "outputId": "ac27605d-b153-421f-9ff2-6c1f5a19546c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2024.12.3: Fast Llama patching. Transformers:4.46.3.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d355bfd2795e47018fb303c4b19e818b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.12.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>HELLO THERE I AM 1-800-FLY-ME-TO-SPACE\n",
            "It's time to take a trip to the stars. You've always dreamed of soaring through the cosmos, and now it's time to make that dream a reality. Welcome to 1-800-FLY-ME-TO-SPACE, the premier intergalactic travel agency.\n",
            "\n",
            "**Destination Options**\n",
            "\n",
            "* **Mars**: Explore the red planet's rust-red landscape, visit ancient riverbeds, and marvel at the towering volcano Olympus Mons. (3-day trip, $50,000 per person)\n",
            "* **The Moon**: Walk on the lunar surface, collect moon\n"
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = saved_model_name, # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    # alpaca_prompt.format(\n",
        "    #     \"What is a famous tall tower in Paris?\", # instruction\n",
        "    #     \"\", # input\n",
        "    #     \"\", # output - leave this blank for generation!\n",
        "    # )\n",
        "    \"HELLO THERE I AM \"\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0f8a1aae8fc5478fa696044f2c0ff94c",
            "f2774ad2b5c443bca27e555a65b540f9",
            "45fe42d5f00a46d48842d90324766c09",
            "46409c5e2d9a426fbe4e4a25c3f3bb91",
            "90d4a797a3b54d929c34201cd1e1a02b",
            "e3980e7d73124b71b19b3654f7d6eabc",
            "d570419bd79c4cc6a56cc62318a585ab",
            "75292f465998461cbc7584e1c573fb0a",
            "2139bebdb2834d59b508d88da58ca17e",
            "f4392f48c7824647923a31c6d875c3d7",
            "16373e55f4b645d481b7cbdc5e405d56",
            "7b5687bf8cd94fccbde30658b1855237",
            "a61af8a5d62f4e5f9519fd121ed719e8",
            "136c7baa37084074b62fe532cfccf478",
            "ea4b16ac378445d593ccbd819999a68a",
            "095c519b46dd4964893f6db1afc9e5ed",
            "6ca8ce142aed476a883ad7eb13222185",
            "980b536a0c2848758dc1d283c2a61042",
            "58f613edddee4b5981822338ec6d3d0d",
            "610d99411edd463fb1520979e73d7012",
            "12af26c2cb404fa5941aa40a442545d9",
            "6aeb34a8930d492bac0766755f44c3b3"
          ]
        },
        "outputId": "3f361b17-8e11-4c11-8e35-db65cdc6f9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 2.2G\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 0.76 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [00:01<00:00, 17.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/pytorch_model-00001-of-00002.bin...\n",
            "Unsloth: Saving AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/pytorch_model-00002-of-00002.bin...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF into f16 GGUF format.\n",
            "The output location will be /content/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/unsloth.F16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: Llama-3.2_3B_C242-ET01_GGUF\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {64}\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00002.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> F16, shape = {3072, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00002.bin'\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> F16, shape = {3072, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> F16, shape = {3072, 3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> F16, shape = {3072, 8192}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> F16, shape = {8192, 3072}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {3072}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 131072\n",
            "INFO:hf-to-gguf:gguf: embedding length = 3072\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
            "INFO:hf-to-gguf:gguf: head count = 24\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 1\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting special token type pad to 128004\n",
            "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- if strftime_now is defined %}\n",
            "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
            "    {%- else %}\n",
            "        {%- set date_string = \"26 Jul 2024\" %}\n",
            "    {%- endif %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "        {{- '\"parameters\": ' }}\n",
            "        {{- tool_call.arguments | tojson }}\n",
            "        {{- \"}\" }}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/unsloth.F16.gguf: n_tensors = 255, total_size = 6.4G\n",
            "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.43G/6.43G [07:06<00:00, 15.1Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/unsloth.F16.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/unsloth.F16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 4280 (3df784b3)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing '/content/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/unsloth.F16.gguf' to '/content/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/unsloth.Q4_K_M.gguf' as Q4_K_M using 4 threads\n",
            "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /content/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/unsloth.F16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct Bnb 4bit\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct-bnb-4bit\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"ƒ† ƒ†\", \"ƒ† ƒ†ƒ†ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n",
            "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type  f16:  197 tensors\n",
            "[   1/ 255]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   2/ 255]                    rope_freqs.weight - [   64,     1,     1,     1], type =    f32, size =    0.000 MB\n",
            "[   3/ 255]                    token_embd.weight - [ 3072, 128256,     1,     1], type =    f16, converting to q6_K .. size =   751.50 MiB ->   308.23 MiB\n",
            "[   4/ 255]                  blk.0.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[   5/ 255]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[   6/ 255]             blk.0.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   7/ 255]                  blk.0.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[   8/ 255]                  blk.0.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[   9/ 255]                blk.0.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  10/ 255]                blk.0.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  11/ 255]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  12/ 255]                  blk.0.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  13/ 255]                  blk.1.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  14/ 255]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  15/ 255]             blk.1.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  16/ 255]                  blk.1.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  17/ 255]                  blk.1.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  18/ 255]                blk.1.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  19/ 255]                blk.1.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  20/ 255]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  21/ 255]                  blk.1.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  22/ 255]                  blk.2.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  23/ 255]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  24/ 255]             blk.2.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  25/ 255]                  blk.2.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  26/ 255]                  blk.2.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  27/ 255]                blk.2.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  28/ 255]                blk.2.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  29/ 255]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  30/ 255]                  blk.2.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  31/ 255]                  blk.3.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  32/ 255]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  33/ 255]             blk.3.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  34/ 255]                  blk.3.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  35/ 255]                  blk.3.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  36/ 255]                blk.3.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  37/ 255]                blk.3.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  38/ 255]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  39/ 255]                  blk.3.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  40/ 255]                  blk.4.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  41/ 255]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  42/ 255]             blk.4.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  43/ 255]                  blk.4.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  44/ 255]                  blk.4.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  45/ 255]                blk.4.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  46/ 255]                blk.4.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  47/ 255]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  48/ 255]                  blk.4.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  49/ 255]                  blk.5.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  50/ 255]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  51/ 255]             blk.5.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  52/ 255]                  blk.5.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  53/ 255]                  blk.5.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  54/ 255]                blk.5.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  55/ 255]                blk.5.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  56/ 255]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  57/ 255]                  blk.5.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  58/ 255]                  blk.6.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  59/ 255]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  60/ 255]             blk.6.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  61/ 255]                  blk.6.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  62/ 255]                  blk.6.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  63/ 255]                blk.6.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  64/ 255]                blk.6.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  65/ 255]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  66/ 255]                  blk.6.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  67/ 255]                  blk.7.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  68/ 255]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  69/ 255]             blk.7.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  70/ 255]                  blk.7.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  71/ 255]                  blk.7.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  72/ 255]                blk.7.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  73/ 255]                blk.7.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  74/ 255]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  75/ 255]                  blk.7.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  76/ 255]                  blk.8.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  77/ 255]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  78/ 255]             blk.8.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  79/ 255]                  blk.8.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  80/ 255]                  blk.8.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[  81/ 255]                blk.8.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[  82/ 255]                blk.8.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  83/ 255]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  84/ 255]                  blk.8.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  85/ 255]                  blk.9.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  86/ 255]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  87/ 255]             blk.9.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  88/ 255]                  blk.9.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  89/ 255]                  blk.9.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  90/ 255]                blk.9.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  91/ 255]                blk.9.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  92/ 255]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  93/ 255]                  blk.9.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[  94/ 255]                 blk.10.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  95/ 255]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[  96/ 255]            blk.10.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  97/ 255]                 blk.10.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[  98/ 255]                 blk.10.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[  99/ 255]               blk.10.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 100/ 255]               blk.10.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 101/ 255]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 102/ 255]                 blk.10.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 103/ 255]                 blk.11.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 104/ 255]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 105/ 255]            blk.11.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 106/ 255]                 blk.11.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 107/ 255]                 blk.11.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 108/ 255]               blk.11.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 109/ 255]               blk.11.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 110/ 255]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 111/ 255]                 blk.11.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 112/ 255]                 blk.12.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 113/ 255]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 114/ 255]            blk.12.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 115/ 255]                 blk.12.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 116/ 255]                 blk.12.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 117/ 255]               blk.12.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 118/ 255]               blk.12.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 119/ 255]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 120/ 255]                 blk.12.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 121/ 255]                 blk.13.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 122/ 255]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 123/ 255]            blk.13.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 124/ 255]                 blk.13.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 125/ 255]                 blk.13.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 126/ 255]               blk.13.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 127/ 255]               blk.13.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 128/ 255]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 129/ 255]                 blk.13.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 130/ 255]                 blk.14.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 131/ 255]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 132/ 255]            blk.14.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 133/ 255]                 blk.14.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 134/ 255]                 blk.14.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 135/ 255]               blk.14.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 136/ 255]               blk.14.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 137/ 255]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 138/ 255]                 blk.14.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 139/ 255]                 blk.15.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 140/ 255]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 141/ 255]            blk.15.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 142/ 255]                 blk.15.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 143/ 255]                 blk.15.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 144/ 255]               blk.15.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 145/ 255]               blk.15.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 146/ 255]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 147/ 255]                 blk.15.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 148/ 255]                 blk.16.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 149/ 255]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 150/ 255]            blk.16.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 151/ 255]                 blk.16.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 152/ 255]                 blk.16.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 153/ 255]               blk.16.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 154/ 255]               blk.16.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 155/ 255]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 156/ 255]                 blk.16.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 157/ 255]                 blk.17.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 158/ 255]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 159/ 255]            blk.17.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 160/ 255]                 blk.17.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 161/ 255]                 blk.17.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 162/ 255]               blk.17.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 163/ 255]               blk.17.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 164/ 255]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 165/ 255]                 blk.17.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 166/ 255]                 blk.18.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 167/ 255]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 168/ 255]            blk.18.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 169/ 255]                 blk.18.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 170/ 255]                 blk.18.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 171/ 255]               blk.18.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 172/ 255]               blk.18.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 173/ 255]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 174/ 255]                 blk.18.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 175/ 255]                 blk.19.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 176/ 255]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 177/ 255]            blk.19.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 178/ 255]                 blk.19.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 179/ 255]                 blk.19.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 180/ 255]               blk.19.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 181/ 255]               blk.19.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 182/ 255]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 183/ 255]                 blk.19.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 184/ 255]                 blk.20.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 185/ 255]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 186/ 255]            blk.20.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 187/ 255]                 blk.20.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 188/ 255]                 blk.20.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 189/ 255]               blk.20.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 190/ 255]               blk.20.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 191/ 255]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 192/ 255]                 blk.20.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 193/ 255]                 blk.21.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 194/ 255]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 195/ 255]            blk.21.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 196/ 255]                 blk.21.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 197/ 255]                 blk.21.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 198/ 255]               blk.21.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 199/ 255]               blk.21.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 200/ 255]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 201/ 255]                 blk.21.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 202/ 255]                 blk.22.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 203/ 255]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 204/ 255]            blk.22.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 205/ 255]                 blk.22.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 206/ 255]                 blk.22.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 207/ 255]               blk.22.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 208/ 255]               blk.22.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 209/ 255]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 210/ 255]                 blk.22.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 211/ 255]                 blk.23.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 212/ 255]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 213/ 255]            blk.23.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 214/ 255]                 blk.23.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 215/ 255]                 blk.23.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 216/ 255]               blk.23.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 217/ 255]               blk.23.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 218/ 255]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 219/ 255]                 blk.23.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 220/ 255]                 blk.24.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 221/ 255]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 222/ 255]            blk.24.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 223/ 255]                 blk.24.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 224/ 255]                 blk.24.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 225/ 255]               blk.24.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 226/ 255]               blk.24.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 227/ 255]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 228/ 255]                 blk.24.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 229/ 255]                 blk.25.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 230/ 255]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 231/ 255]            blk.25.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 232/ 255]                 blk.25.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 233/ 255]                 blk.25.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 234/ 255]               blk.25.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 235/ 255]               blk.25.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 236/ 255]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 237/ 255]                 blk.25.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 238/ 255]                 blk.26.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 239/ 255]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 240/ 255]            blk.26.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 241/ 255]                 blk.26.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 242/ 255]                 blk.26.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 243/ 255]               blk.26.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 244/ 255]               blk.26.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 245/ 255]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 246/ 255]                 blk.26.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 247/ 255]                 blk.27.attn_k.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q4_K .. size =     6.00 MiB ->     1.69 MiB\n",
            "[ 248/ 255]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 249/ 255]            blk.27.attn_output.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 250/ 255]                 blk.27.attn_q.weight - [ 3072,  3072,     1,     1], type =    f16, converting to q4_K .. size =    18.00 MiB ->     5.06 MiB\n",
            "[ 251/ 255]                 blk.27.attn_v.weight - [ 3072,  1024,     1,     1], type =    f16, converting to q6_K .. size =     6.00 MiB ->     2.46 MiB\n",
            "[ 252/ 255]               blk.27.ffn_down.weight - [ 8192,  3072,     1,     1], type =    f16, converting to q6_K .. size =    48.00 MiB ->    19.69 MiB\n",
            "[ 253/ 255]               blk.27.ffn_gate.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "[ 254/ 255]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
            "[ 255/ 255]                 blk.27.ffn_up.weight - [ 3072,  8192,     1,     1], type =    f16, converting to q4_K .. size =    48.00 MiB ->    13.50 MiB\n",
            "llama_model_quantize_internal: model size  =  6128.17 MB\n",
            "llama_model_quantize_internal: quant size  =  1918.35 MB\n",
            "\n",
            "main: quantize time = 658046.63 ms\n",
            "main:    total time = 658046.63 ms\n",
            "Unsloth: Conversion completed! Output location: /content/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF/unsloth.Q4_K_M.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f8a1aae8fc5478fa696044f2c0ff94c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsloth.Q4_K_M.gguf:   0%|          | 0.00/2.02G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b5687bf8cd94fccbde30658b1855237"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved GGUF to https://huggingface.co/AscendingGrass/Llama-3.2_3B_C242-ET01_GGUF\n"
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if True:\n",
        "    from google.colab import userdata\n",
        "    model.push_to_hub_gguf(\n",
        "        saved_model_name+\"_GGUF\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\"],\n",
        "        token = userdata.get(\"HF_PUSH_TOKEN\"),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDp0zNpwe6U_"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in `llama.cpp` or a UI based system like `GPT4All`. You can install GPT4All by going [here](https://gpt4all.io/index.html).\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/drive/1T-YBVfnphoVc8E2E854qF3jdia2Ll2W2?usp=sharing)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with ü§ó HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "10. [**NEW**] We make Gemma-2 9b / 27b **2x faster**! See our [Gemma-2 9b notebook](https://colab.research.google.com/drive/1vIrqH5uYDQwsJ4-OO3DErvuv4pBgVwk4?usp=sharing)\n",
        "11. [**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/drive/1WZDi7APtQ9VsvOrQSSC5DDtxq159j8iZ?usp=sharing)\n",
        "12. [**NEW**] We make Mistral NeMo 12B 2x faster and fit in under 12GB of VRAM! [Mistral NeMo notebook](https://colab.research.google.com/drive/17d3U-CAIwzmbDRqbZ9NnpHxCkmXB6LZ0?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Support our work if you can! Thanks!\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a6065ab8a73847ec9c845f40bb0f1757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec15cecda6064d459e4ff317a2614b8b",
              "IPY_MODEL_0349b41a0c3e489f87ac753edd48fad3",
              "IPY_MODEL_14980788acca42b18ebe8d5c66047b46"
            ],
            "layout": "IPY_MODEL_6f48269bbd3f4fa5a68ea5f9b422ae78"
          }
        },
        "ec15cecda6064d459e4ff317a2614b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a086aaec5a3643c38d0bd49ca3804f34",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_44ad172cb4084ece94712960cba91daf",
            "value": "README.md:‚Äá100%"
          }
        },
        "0349b41a0c3e489f87ac753edd48fad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_683a7cba669f45b1b490b400a0fd688a",
            "max": 603,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9085cc7419d9451eb5cefb13ff2a6b13",
            "value": 603
          }
        },
        "14980788acca42b18ebe8d5c66047b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2169bef564a54629be459cd18a55fbe0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9acbe1fd69294e6aaf25d0ad8594742d",
            "value": "‚Äá603/603‚Äá[00:00&lt;00:00,‚Äá36.1kB/s]"
          }
        },
        "6f48269bbd3f4fa5a68ea5f9b422ae78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a086aaec5a3643c38d0bd49ca3804f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44ad172cb4084ece94712960cba91daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "683a7cba669f45b1b490b400a0fd688a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9085cc7419d9451eb5cefb13ff2a6b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2169bef564a54629be459cd18a55fbe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9acbe1fd69294e6aaf25d0ad8594742d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d1e90a8554a4ce8986797e829df2822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6883d84680c4716b7c2b45921fffdf8",
              "IPY_MODEL_9dfc5badcf7747e9a5a2caf6d9744692",
              "IPY_MODEL_27e847a0657b4712ae4cc197f99e4e95"
            ],
            "layout": "IPY_MODEL_ea842a8a52d8401e858a485fdd7a0531"
          }
        },
        "c6883d84680c4716b7c2b45921fffdf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf31cfaf8e2544d4b4cf541ef92a63da",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_36a5c96e6c3c4c69a8c517a89822bb42",
            "value": "100%"
          }
        },
        "9dfc5badcf7747e9a5a2caf6d9744692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8714c6864484e42bc771749910d8f75",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8446e0526f534be18c39f1ea9f36794d",
            "value": 1
          }
        },
        "27e847a0657b4712ae4cc197f99e4e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_508ceda9d28448c4b60983aa6b5e78af",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a0ccbf04155d4ccea78afa0fdaa7d070",
            "value": "‚Äá1/1‚Äá[00:01&lt;00:00,‚Äá‚Äá1.88s/it]"
          }
        },
        "ea842a8a52d8401e858a485fdd7a0531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf31cfaf8e2544d4b4cf541ef92a63da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36a5c96e6c3c4c69a8c517a89822bb42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8714c6864484e42bc771749910d8f75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8446e0526f534be18c39f1ea9f36794d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "508ceda9d28448c4b60983aa6b5e78af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ccbf04155d4ccea78afa0fdaa7d070": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e06d0a54f43430497b74e884ce0beb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_159f4a611d8644cf8df08aa5fa319b9d",
              "IPY_MODEL_f85835c0418148b292e18224084fee39",
              "IPY_MODEL_58fb208e819a41388041b8efba349219"
            ],
            "layout": "IPY_MODEL_5b84fbcc0bf44b3186de04c16c8b7f89"
          }
        },
        "159f4a611d8644cf8df08aa5fa319b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_214ece7fb1414f0a99647073a5cfe6f9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1481bb391798485bbfd272df95ad3f64",
            "value": "adapter_model.safetensors:‚Äá"
          }
        },
        "f85835c0418148b292e18224084fee39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_866c3c9bd00e4d2eaa61c8633c5af31f",
            "max": 97307544,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3e2e6501f7e4ebf9450b8ae772a1ab8",
            "value": 97307544
          }
        },
        "58fb208e819a41388041b8efba349219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5709dabc11d643f18ee6e35209ac9b2f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_496c614a51204cb0915075e03f46d35f",
            "value": "‚Äá112M/?‚Äá[00:01&lt;00:00,‚Äá110MB/s]"
          }
        },
        "5b84fbcc0bf44b3186de04c16c8b7f89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214ece7fb1414f0a99647073a5cfe6f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1481bb391798485bbfd272df95ad3f64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "866c3c9bd00e4d2eaa61c8633c5af31f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3e2e6501f7e4ebf9450b8ae772a1ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5709dabc11d643f18ee6e35209ac9b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "496c614a51204cb0915075e03f46d35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77ba741a2f924864a6bfe885f4155637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51ba12b39d774d1aa693573ce715d1c6",
              "IPY_MODEL_df64b328aabc41eabe6c13e2372e3752",
              "IPY_MODEL_f677933bb4594481aa2d53894eed91d2"
            ],
            "layout": "IPY_MODEL_f1de4d8520374823a63f83fba44cbd52"
          }
        },
        "51ba12b39d774d1aa693573ce715d1c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2ac1cbb86214038b0cc9465319ccbad",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ef39eb4d8c75467d875670742e47a2bf",
            "value": "100%"
          }
        },
        "df64b328aabc41eabe6c13e2372e3752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f1ed120a76747b1837ecdd87f2e2cd9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76dc7c67f3f748e689a8f8ccc3d15d17",
            "value": 1
          }
        },
        "f677933bb4594481aa2d53894eed91d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1be5dad10647480eb8d2ed7a5c4ba6b7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8e9be764eb8243158726fc75419d4b22",
            "value": "‚Äá1/1‚Äá[00:01&lt;00:00,‚Äá‚Äá1.18s/it]"
          }
        },
        "f1de4d8520374823a63f83fba44cbd52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2ac1cbb86214038b0cc9465319ccbad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef39eb4d8c75467d875670742e47a2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f1ed120a76747b1837ecdd87f2e2cd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76dc7c67f3f748e689a8f8ccc3d15d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1be5dad10647480eb8d2ed7a5c4ba6b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e9be764eb8243158726fc75419d4b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b32fde2393d42c982f97ab33f3f5f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce5f3ad9eb5a44ed8a50389fc1121fa2",
              "IPY_MODEL_68dab5985f964d3bb5c4de0246824e85",
              "IPY_MODEL_807180bccb774df8ac40a0ed6e4cbdd6"
            ],
            "layout": "IPY_MODEL_97eff5ab531e4c22a503274ebf178d6f"
          }
        },
        "ce5f3ad9eb5a44ed8a50389fc1121fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d7379a8d2654233a8175032ad985196",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_16fe301a1b934b20884dc6ce1bea32b8",
            "value": "tokenizer.json:‚Äá"
          }
        },
        "68dab5985f964d3bb5c4de0246824e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b3a831bc32943dd90f5eb012cccd3bf",
            "max": 17209920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_925a769a27db4979bb0ab18b311c0de9",
            "value": 17209920
          }
        },
        "807180bccb774df8ac40a0ed6e4cbdd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da1b40b7ae0e46d2b96bd62be3216d50",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d58939c751884a60a06c734391f0cf3a",
            "value": "‚Äá32.0M/?‚Äá[00:00&lt;00:00,‚Äá33.6MB/s]"
          }
        },
        "97eff5ab531e4c22a503274ebf178d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d7379a8d2654233a8175032ad985196": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16fe301a1b934b20884dc6ce1bea32b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b3a831bc32943dd90f5eb012cccd3bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "925a769a27db4979bb0ab18b311c0de9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da1b40b7ae0e46d2b96bd62be3216d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d58939c751884a60a06c734391f0cf3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d355bfd2795e47018fb303c4b19e818b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01b378cd8347424bb7803b532890e267",
              "IPY_MODEL_69f4879407ef431192ef98288b7a090c",
              "IPY_MODEL_e9c876e3ece5492b9b4bc6e58f61556f"
            ],
            "layout": "IPY_MODEL_f05b456fdd934f2dbcc354a336a9ac50"
          }
        },
        "01b378cd8347424bb7803b532890e267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b79e193b15540e49b9ef7285e750512",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5493b0c290b04c7fb65a41438bcd59f4",
            "value": "adapter_model.safetensors:‚Äá100%"
          }
        },
        "69f4879407ef431192ef98288b7a090c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b802be75d854969823d3c734154df17",
            "max": 167832240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10b53a6ab9e44513b493294496c9819e",
            "value": 167832224
          }
        },
        "e9c876e3ece5492b9b4bc6e58f61556f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ce87a860db47829e1d4bf6220a12ca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_215a911336994f0c9af7429309bc2f1c",
            "value": "‚Äá168M/168M‚Äá[00:01&lt;00:00,‚Äá188MB/s]"
          }
        },
        "f05b456fdd934f2dbcc354a336a9ac50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b79e193b15540e49b9ef7285e750512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5493b0c290b04c7fb65a41438bcd59f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b802be75d854969823d3c734154df17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10b53a6ab9e44513b493294496c9819e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88ce87a860db47829e1d4bf6220a12ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "215a911336994f0c9af7429309bc2f1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f8a1aae8fc5478fa696044f2c0ff94c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2774ad2b5c443bca27e555a65b540f9",
              "IPY_MODEL_45fe42d5f00a46d48842d90324766c09",
              "IPY_MODEL_46409c5e2d9a426fbe4e4a25c3f3bb91"
            ],
            "layout": "IPY_MODEL_90d4a797a3b54d929c34201cd1e1a02b"
          }
        },
        "f2774ad2b5c443bca27e555a65b540f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3980e7d73124b71b19b3654f7d6eabc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d570419bd79c4cc6a56cc62318a585ab",
            "value": "100%"
          }
        },
        "45fe42d5f00a46d48842d90324766c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75292f465998461cbc7584e1c573fb0a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2139bebdb2834d59b508d88da58ca17e",
            "value": 1
          }
        },
        "46409c5e2d9a426fbe4e4a25c3f3bb91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4392f48c7824647923a31c6d875c3d7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_16373e55f4b645d481b7cbdc5e405d56",
            "value": "‚Äá1/1‚Äá[00:20&lt;00:00,‚Äá20.39s/it]"
          }
        },
        "90d4a797a3b54d929c34201cd1e1a02b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3980e7d73124b71b19b3654f7d6eabc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d570419bd79c4cc6a56cc62318a585ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75292f465998461cbc7584e1c573fb0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2139bebdb2834d59b508d88da58ca17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4392f48c7824647923a31c6d875c3d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16373e55f4b645d481b7cbdc5e405d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b5687bf8cd94fccbde30658b1855237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a61af8a5d62f4e5f9519fd121ed719e8",
              "IPY_MODEL_136c7baa37084074b62fe532cfccf478",
              "IPY_MODEL_ea4b16ac378445d593ccbd819999a68a"
            ],
            "layout": "IPY_MODEL_095c519b46dd4964893f6db1afc9e5ed"
          }
        },
        "a61af8a5d62f4e5f9519fd121ed719e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ca8ce142aed476a883ad7eb13222185",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_980b536a0c2848758dc1d283c2a61042",
            "value": "unsloth.Q4_K_M.gguf:‚Äá"
          }
        },
        "136c7baa37084074b62fe532cfccf478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58f613edddee4b5981822338ec6d3d0d",
            "max": 2019377248,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_610d99411edd463fb1520979e73d7012",
            "value": 2019377248
          }
        },
        "ea4b16ac378445d593ccbd819999a68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12af26c2cb404fa5941aa40a442545d9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6aeb34a8930d492bac0766755f44c3b3",
            "value": "‚Äá2.03G/?‚Äá[00:20&lt;00:00,‚Äá1.35GB/s]"
          }
        },
        "095c519b46dd4964893f6db1afc9e5ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ca8ce142aed476a883ad7eb13222185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "980b536a0c2848758dc1d283c2a61042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58f613edddee4b5981822338ec6d3d0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610d99411edd463fb1520979e73d7012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "12af26c2cb404fa5941aa40a442545d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6aeb34a8930d492bac0766755f44c3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}